{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Installing Libraries","metadata":{}},{"cell_type":"code","source":"!pip install nltk\n!pip install transformers --quiet\n\nimport re\nimport nltk\nimport string\nimport os, gc\nimport pandas as pd\nimport tensorflow as tf\nfrom transformers import TFAutoModel\nfrom transformers import AutoTokenizer\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2023-05-02T11:11:58.364454Z","iopub.execute_input":"2023-05-02T11:11:58.365337Z","iopub.status.idle":"2023-05-02T11:12:09.831663Z","shell.execute_reply.started":"2023-05-02T11:11:58.365302Z","shell.execute_reply":"2023-05-02T11:12:09.830520Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.8/site-packages (3.8.1)\nRequirement already satisfied: click in /usr/local/lib/python3.8/site-packages (from nltk) (8.1.3)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/site-packages (from nltk) (2023.3.23)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from nltk) (4.65.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.8/site-packages (from nltk) (1.2.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"## Setting data paths","metadata":{}},{"cell_type":"code","source":"main_data_dir_path = \"../input/jigsaw-multilingual-toxic-comment-classification/\"\ntoxic_comment_train_csv_path = main_data_dir_path + \"jigsaw-toxic-comment-train.csv\"\nunintended_bias_train_csv_path = main_data_dir_path + \"jigsaw-unintended-bias-train.csv\"\nvalidation_csv_path = main_data_dir_path + \"validation.csv\"\ntest_csv_path = main_data_dir_path + \"test.csv\"\nsubmission_csv_path = main_data_dir_path + \"sample_submission.csv\"","metadata":{"execution":{"iopub.status.busy":"2023-05-02T11:12:19.756819Z","iopub.execute_input":"2023-05-02T11:12:19.757245Z","iopub.status.idle":"2023-05-02T11:12:19.763301Z","shell.execute_reply.started":"2023-05-02T11:12:19.757215Z","shell.execute_reply":"2023-05-02T11:12:19.762354Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## TPU Configurations","metadata":{}},{"cell_type":"code","source":"#################### TPU Configurations ####################\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\nAUTO = tf.data.experimental.AUTOTUNE\n# Configuration\nEPOCHS = 2\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 192\nMODEL = 'bert-base-multilingual-uncased'\nNUM_SAMPLES = 150000\nRANDOM_STATE = 42\nLEARNING_RATE = 2e-5\nWEIGHT_DECAY = 1e-6","metadata":{"execution":{"iopub.status.busy":"2023-05-02T11:12:23.105804Z","iopub.execute_input":"2023-05-02T11:12:23.106232Z","iopub.status.idle":"2023-05-02T11:12:32.765169Z","shell.execute_reply.started":"2023-05-02T11:12:23.106201Z","shell.execute_reply":"2023-05-02T11:12:32.764093Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Running on TPU  \nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\nINFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\nREPLICAS:  8\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Reading & Balancing the data by Target column","metadata":{}},{"cell_type":"code","source":"## Reading csv files \ntrain1 = pd.read_csv(toxic_comment_train_csv_path)\ntrain2 = pd.read_csv(unintended_bias_train_csv_path)\nvalid = pd.read_csv(validation_csv_path)\ntest = pd.read_csv(test_csv_path)\nsub = pd.read_csv(submission_csv_path)\n\n## Converting floating points to integers ##\ntrain2.toxic = train2['toxic'].round().astype(int)\n\n##### BALANCING THE DATA ##### \n# : Taking all the data from toxic_comment_train_file & all data corresponding to unintended bias train file\n# & sampling 150k observations randomly from non-toxic observation population.\n\n# Combine train1 with a subset of train2\ntrain = pd.concat([\n    train1[['comment_text', 'toxic']],\n    train2[['comment_text', 'toxic']].query('toxic==1'),\n    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=NUM_SAMPLES, random_state=RANDOM_STATE)\n])\n\n## Dropping missing observations with respect to comment-text column \ntrain = train.dropna(subset=['comment_text'])","metadata":{"execution":{"iopub.status.busy":"2023-05-02T11:12:43.876484Z","iopub.execute_input":"2023-05-02T11:12:43.877200Z","iopub.status.idle":"2023-05-02T11:13:02.198406Z","shell.execute_reply.started":"2023-05-02T11:12:43.877161Z","shell.execute_reply":"2023-05-02T11:13:02.197006Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing the data","metadata":{}},{"cell_type":"code","source":"## Removing URL's\ndef remove_urls(text):\n    \"\"\"\n    Function takes a sentence and removes the URLs from the provided sentence.\n    \"\"\"\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\n# define a regular expression pattern to match non-alphanumeric characters\npattern = r\"[^a-zA-Z0-9 ]\"\n\n# define a function to remove non-alphanumeric characters from a string using the pattern\ndef remove_non_alphanumeric(text):\n    return re.sub(pattern, \"\", text)\n\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text: str, punctuations: str):\n    \"\"\"\n    Function takes a sentence & a punctuations string to remove punctuations from a sentence.\n    \"\"\"\n    return text.translate(str.maketrans(\"\", \"\", punctuations))\n\n## Removing multi-characters\ndef remove_multiplechars(text):\n    text = re.sub(r'(.)\\1{3,}',r'\\1', text)\n    return text\n\ndef contraction_to_expansion(text: str, contractions_dict: dict):\n    \"\"\"\n    Function takes a sentence (text) and a dictionary to map words like ain't to am not etc.\n    and returns the final sentence.\n    \"\"\"\n    if type(text) is str:\n        for key in contractions_dict:\n            value = contractions_dict[key]\n            text = text.replace(key, value)\n        return text\n    else:\n        return text\n    \ncontractions = {\"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\n                \"'cause\": \"because\",\"could've\": \"could have\", \"couldn't\": \"could not\",\n                \"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\n                \"hadn't\": \"had not\", \"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\n                \"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n                \"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\n                \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\n                \"I've\": \"I have\",\"isn't\": \"is not\",\"it'd\": \"it had\", \"it'd've\": \"it would have\",\"it'll\": \"it will\",\n                \"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n                \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\n                \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n                \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\", \n                \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n                \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\", \"she'll\": \"she will\",\n                \"she'll've\": \"she will have\",\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\n                \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\",\n                \"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there had\",\n                \"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\n                \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\n                \"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\",\n                \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\n                \"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\", \"what'll've\": \"what will have\",\n                \"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\n                \"when've\": \"when have\", \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\n                \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\", \"who've\": \"who have\",\n                \"why's\": \"why is\",\"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n                \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n                \"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\n                \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n                \"you'd\": \"you had\", \"you'd've\": \"you would have\",\"you'll\": \"you you will\",\n                \"you'll've\": \"you you will have\",\"you're\": \"you are\",\"you've\": \"you have\"}\n\n#################################################### CLEANING THE DATA ####################################################\ndef clean_text(text):\n    text = text.replace(\"\\n\", \"\") ## Remove the next line character ##\n    text = remove_urls(text) ## Remove the URLs ##\n    text = re.compile(r'ï¼¼\\(.+?\\)/').sub(\"\", text) ## Remove emoticons ##\n    text = remove_non_alphanumeric(text) ## Remove Unicode characters ##\n    text = remove_punctuation(text, PUNCT_TO_REMOVE) ## Removing Punctuations ##\n    text = remove_multiplechars(text) ## Removing multiple characters in a word ##\n    text = contraction_to_expansion(text, contractions)\n    return text\n\nstop_words = nltk.corpus.stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2023-05-02T11:13:10.245359Z","iopub.execute_input":"2023-05-02T11:13:10.245831Z","iopub.status.idle":"2023-05-02T11:13:10.272283Z","shell.execute_reply.started":"2023-05-02T11:13:10.245802Z","shell.execute_reply":"2023-05-02T11:13:10.271349Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"###### CLEANING THE DATA ######\n\ntrain['comment_text'] = train['comment_text'].apply(lambda x: clean_text(x))\nvalid['comment_text'] = valid['comment_text'].apply(lambda x: clean_text(x))\ntest['content'] = test['content'].apply(lambda x: clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2023-05-02T11:13:12.555321Z","iopub.execute_input":"2023-05-02T11:13:12.555785Z","iopub.status.idle":"2023-05-02T11:14:28.283285Z","shell.execute_reply.started":"2023-05-02T11:13:12.555732Z","shell.execute_reply":"2023-05-02T11:14:28.281836Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def encode(texts, tokenizer, max_len):\n    \"\"\"\n    Function takes a list of texts, tokenizer (object)\n    initialized from HuggingFace library, max_len (defines\n    of how long the sentence lengths should be).\n    \"\"\"       \n    tokens = tokenizer(texts, max_length=max_len, \n                    truncation=True, padding='max_length',\n                    add_special_tokens=True, return_tensors='np')\n    \n    return tokens","metadata":{"execution":{"iopub.status.busy":"2023-05-02T11:14:30.589808Z","iopub.execute_input":"2023-05-02T11:14:30.590258Z","iopub.status.idle":"2023-05-02T11:14:30.596642Z","shell.execute_reply.started":"2023-05-02T11:14:30.590203Z","shell.execute_reply":"2023-05-02T11:14:30.595617Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Encoding comment_text","metadata":{}},{"cell_type":"code","source":"## Intializing the tokenizer ##\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\ntrain_inputs = encode(train['comment_text'].values.tolist(), \n                      tokenizer, max_len=MAX_LEN)\nvalidation_inputs = encode(valid['comment_text'].values.tolist(),\n                          tokenizer, max_len=MAX_LEN)\ntest_inputs = encode(test['content'].values.tolist(),\n                    tokenizer, max_len=MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T11:14:33.510708Z","iopub.execute_input":"2023-05-02T11:14:33.511212Z","iopub.status.idle":"2023-05-02T11:15:38.557884Z","shell.execute_reply.started":"2023-05-02T11:14:33.511175Z","shell.execute_reply":"2023-05-02T11:15:38.556475Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Preparing data using tf.data.Data API","metadata":{}},{"cell_type":"code","source":"def map_fn(input_ids, attention_mask, labels=None):\n    if labels is not None:\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}, labels\n    else:\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}","metadata":{"execution":{"iopub.status.busy":"2023-05-02T11:15:41.465837Z","iopub.execute_input":"2023-05-02T11:15:41.466827Z","iopub.status.idle":"2023-05-02T11:15:41.472165Z","shell.execute_reply.started":"2023-05-02T11:15:41.466789Z","shell.execute_reply":"2023-05-02T11:15:41.471107Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs[\"input_ids\"],\n                                                    train_inputs[\"attention_mask\"],\n                                                   train['toxic']))\ntrain_dataset = train_dataset.map(map_fn)\ntrain_dataset = train_dataset.repeat().shuffle(2048).batch(BATCH_SIZE).prefetch(AUTO)\n\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((validation_inputs['input_ids'],\n                                                         validation_inputs['attention_mask'],\n                                                        valid['toxic']))\nvalidation_dataset = validation_dataset.map(map_fn)\nvalidation_dataset = validation_dataset.batch(BATCH_SIZE).prefetch(AUTO)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_inputs['input_ids'],\n                                                  test_inputs['attention_mask']))\ntest_dataset = test_dataset.map(map_fn)\ntest_dataset = test_dataset.batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T11:15:42.815576Z","iopub.execute_input":"2023-05-02T11:15:42.816436Z","iopub.status.idle":"2023-05-02T11:15:43.652516Z","shell.execute_reply.started":"2023-05-02T11:15:42.816403Z","shell.execute_reply":"2023-05-02T11:15:43.651079Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Building the model","metadata":{}},{"cell_type":"code","source":"def build_model(transformer_layer, max_len):\n    \"\"\"\n    Creating the model input layers, output layers,\n    model definition and compilation.\n        \n    Returns: model object after compiling. \n    \"\"\"\n    input_ids = tf.keras.layers.Input(shape=(max_len,), \n                                      dtype=tf.int32, \n                                      name=\"input_ids\")\n    attention_mask = tf.keras.layers.Input(shape=(max_len,), \n                                       dtype=tf.int32, \n                                       name=\"attention_mask\")\n    embeddings = transformer_layer(input_ids, \n                                 attention_mask=attention_mask)[1]\n    x = tf.keras.layers.Dense(1024, activation='relu')(embeddings)\n    y = tf.keras.layers.Dense(1, activation='sigmoid',name='outputs')(x)\n    model = tf.keras.models.Model(inputs=[input_ids, attention_mask],\n                             outputs=y)\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE,\n                                         weight_decay=WEIGHT_DECAY)\n    loss = tf.keras.losses.BinaryCrossentropy()\n    AUC = tf.keras.metrics.AUC()\n    \n    model.compile(loss=loss, metrics=[AUC], optimizer=optimizer)    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-05-02T11:15:45.745595Z","iopub.execute_input":"2023-05-02T11:15:45.746341Z","iopub.status.idle":"2023-05-02T11:15:45.755882Z","shell.execute_reply.started":"2023-05-02T11:15:45.746303Z","shell.execute_reply":"2023-05-02T11:15:45.754818Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Loading model on TPUs","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer,\n                        max_len=MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T11:15:51.709309Z","iopub.execute_input":"2023-05-02T11:15:51.710283Z","iopub.status.idle":"2023-05-02T11:16:18.853277Z","shell.execute_reply.started":"2023-05-02T11:15:51.710248Z","shell.execute_reply":"2023-05-02T11:16:18.852079Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training the model on Only English data for 2 epochs","metadata":{}},{"cell_type":"code","source":"train_steps_per_epoch = train_inputs['input_ids'].shape[0] // BATCH_SIZE\ntrain_history = model.fit(train_dataset,\n                         steps_per_epoch=train_steps_per_epoch,\n                         validation_data=validation_dataset,\n                         epochs=2) ","metadata":{"execution":{"iopub.status.busy":"2023-05-02T11:16:20.815317Z","iopub.execute_input":"2023-05-02T11:16:20.816238Z","iopub.status.idle":"2023-05-02T11:35:13.405837Z","shell.execute_reply.started":"2023-05-02T11:16:20.816203Z","shell.execute_reply":"2023-05-02T11:35:13.404476Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/2\n","output_type":"stream"},{"name":"stderr","text":"2023-05-02 11:17:05.548615: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add_406/ReadVariableOp.\n2023-05-02 11:17:06.548563: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add_406/ReadVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"3795/3795 [==============================] - ETA: 0s - loss: 0.0574 - auc: 0.9967","output_type":"stream"},{"name":"stderr","text":"2023-05-02 11:26:31.256303: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n2023-05-02 11:26:31.539232: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"3795/3795 [==============================] - 623s 138ms/step - loss: 0.0574 - auc: 0.9967 - val_loss: 0.4487 - val_auc: 0.7959\nEpoch 2/2\n3795/3795 [==============================] - 507s 133ms/step - loss: 0.0468 - auc: 0.9977 - val_loss: 0.4806 - val_auc: 0.7645\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training the model on Validation data for 2 epochs further to fine-tune on it","metadata":{}},{"cell_type":"code","source":"validation_steps_per_epoch = validation_inputs['input_ids'].shape[0] // BATCH_SIZE\nvalidation_history = model.fit(validation_dataset.repeat(),\n                              steps_per_epoch=validation_steps_per_epoch,\n                              epochs=2)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T11:36:02.921197Z","iopub.execute_input":"2023-05-02T11:36:02.921671Z","iopub.status.idle":"2023-05-02T11:37:09.771580Z","shell.execute_reply.started":"2023-05-02T11:36:02.921639Z","shell.execute_reply":"2023-05-02T11:37:09.770196Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/2\n62/62 [==============================] - 8s 133ms/step - loss: 0.3284 - auc: 0.8296\nEpoch 2/2\n62/62 [==============================] - 58s 133ms/step - loss: 0.1890 - auc: 0.9542\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Submit to Competition","metadata":{}},{"cell_type":"code","source":"sub['toxic'] = model.predict(test_dataset, verbose=1)\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T11:37:12.920425Z","iopub.execute_input":"2023-05-02T11:37:12.921581Z","iopub.status.idle":"2023-05-02T11:37:49.178010Z","shell.execute_reply.started":"2023-05-02T11:37:12.921539Z","shell.execute_reply":"2023-05-02T11:37:49.176619Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"2023-05-02 11:37:18.431298: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2023-05-02 11:37:18.686189: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"499/499 [==============================] - 36s 48ms/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- Public Leaderboard Score: 0.8259 and Private Leaderboard Score: 0.8239","metadata":{}}]}