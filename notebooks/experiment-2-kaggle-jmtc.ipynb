{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Installing Libraries","metadata":{}},{"cell_type":"code","source":"!pip install transformers --quiet\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom transformers import TFAutoModel\nfrom transformers import AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2023-05-02T10:07:29.443256Z","iopub.execute_input":"2023-05-02T10:07:29.444087Z","iopub.status.idle":"2023-05-02T10:08:04.483065Z","shell.execute_reply.started":"2023-05-02T10:07:29.444043Z","shell.execute_reply":"2023-05-02T10:08:04.482032Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"D0502 10:08:01.435404017    3613 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\nD0502 10:08:01.435440150    3613 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\nD0502 10:08:01.435443598    3613 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\nD0502 10:08:01.435446525    3613 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\nD0502 10:08:01.435448773    3613 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\nD0502 10:08:01.435451080    3613 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\nD0502 10:08:01.435453586    3613 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\nD0502 10:08:01.435456409    3613 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\nD0502 10:08:01.435458571    3613 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\nD0502 10:08:01.435460710    3613 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\nD0502 10:08:01.435462824    3613 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\nD0502 10:08:01.435464910    3613 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\nD0502 10:08:01.435467147    3613 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\nD0502 10:08:01.435469198    3613 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\nI0502 10:08:01.435689796    3613 ev_epoll1_linux.cc:122]               grpc epoll fd: 62\nD0502 10:08:01.435708955    3613 ev_posix.cc:144]                      Using polling engine: epoll1\nD0502 10:08:01.435727635    3613 dns_resolver_ares.cc:822]             Using ares dns resolver\nD0502 10:08:01.445092138    3613 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD0502 10:08:01.445106976    3613 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD0502 10:08:01.445111598    3613 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD0502 10:08:01.445115054    3613 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD0502 10:08:01.445118382    3613 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD0502 10:08:01.445121732    3613 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\nD0502 10:08:01.445129066    3613 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD0502 10:08:01.445148542    3613 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD0502 10:08:01.445180990    3613 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD0502 10:08:01.445199816    3613 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD0502 10:08:01.445204034    3613 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD0502 10:08:01.445207320    3613 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD0502 10:08:01.445213466    3613 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\nD0502 10:08:01.445216946    3613 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD0502 10:08:01.445220283    3613 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD0502 10:08:01.445232596    3613 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\nI0502 10:08:01.447610757    3613 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\nI0502 10:08:01.462380367    3768 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE0502 10:08:01.470802121    3768 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2023-05-02T10:08:01.470783593+00:00\", grpc_status:2}\n/usr/local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Setting data paths","metadata":{}},{"cell_type":"code","source":"main_data_dir_path = \"../input/jigsaw-multilingual-toxic-comment-classification/\"\ntoxic_comment_train_csv_path = main_data_dir_path + \"jigsaw-toxic-comment-train.csv\"\nunintended_bias_train_csv_path = main_data_dir_path + \"jigsaw-unintended-bias-train.csv\"\nvalidation_csv_path = main_data_dir_path + \"validation.csv\"\ntest_csv_path = main_data_dir_path + \"test.csv\"\nsubmission_csv_path = main_data_dir_path + \"sample_submission.csv\"","metadata":{"execution":{"iopub.status.busy":"2023-05-02T10:08:07.258533Z","iopub.execute_input":"2023-05-02T10:08:07.260284Z","iopub.status.idle":"2023-05-02T10:08:07.265429Z","shell.execute_reply.started":"2023-05-02T10:08:07.260239Z","shell.execute_reply":"2023-05-02T10:08:07.264634Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## TPU Configurations","metadata":{}},{"cell_type":"code","source":"#################### TPU Configurations ####################\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n\nAUTO = tf.data.experimental.AUTOTUNE\n# Configuration\nEPOCHS = 2\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 192\nMODEL = 'xlm-roberta-large'\nNUM_SAMPLES = 150000\nRANDOM_STATE = 42\nLEARNING_RATE = 2e-5\nWEIGHT_DECAY = 1e-6","metadata":{"execution":{"iopub.status.busy":"2023-05-02T10:08:09.288948Z","iopub.execute_input":"2023-05-02T10:08:09.289498Z","iopub.status.idle":"2023-05-02T10:08:21.558890Z","shell.execute_reply.started":"2023-05-02T10:08:09.289448Z","shell.execute_reply":"2023-05-02T10:08:21.557832Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Running on TPU  \nINFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\nINFO:tensorflow:Initializing the TPU system: local\nINFO:tensorflow:Finished initializing TPU system.\nINFO:tensorflow:Found TPU system:\nINFO:tensorflow:*** Num TPU Cores: 8\nINFO:tensorflow:*** Num TPU Workers: 1\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\nREPLICAS:  8\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Reading & Balancing the data by Target column","metadata":{}},{"cell_type":"code","source":"## Reading csv files \ntrain1 = pd.read_csv(toxic_comment_train_csv_path)\ntrain2 = pd.read_csv(unintended_bias_train_csv_path)\nvalid = pd.read_csv(validation_csv_path)\ntest = pd.read_csv(test_csv_path)\nsub = pd.read_csv(submission_csv_path)\n\n## Converting floating points to integers ##\ntrain2.toxic = train2['toxic'].round().astype(int)\n\n##### BALANCING THE DATA ##### \n# : Taking all the data from toxic_comment_train_file & all data corresponding to unintended bias train file\n# & sampling 150k observations randomly from non-toxic observation population.\n\n# Combine train1 with a subset of train2\ntrain = pd.concat([\n    train1[['comment_text', 'toxic']],\n    train2[['comment_text', 'toxic']].query('toxic==1'),\n    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=NUM_SAMPLES, random_state=RANDOM_STATE)\n])\n\n## Dropping missing observations with respect to comment-text column \ntrain = train.dropna(subset=['comment_text'])","metadata":{"execution":{"iopub.status.busy":"2023-05-02T10:08:24.958733Z","iopub.execute_input":"2023-05-02T10:08:24.959799Z","iopub.status.idle":"2023-05-02T10:08:42.407588Z","shell.execute_reply.started":"2023-05-02T10:08:24.959752Z","shell.execute_reply":"2023-05-02T10:08:42.406230Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def encode(texts, tokenizer, max_len):\n    \"\"\"\n    Function takes a list of texts, tokenizer (object)\n    initialized from HuggingFace library, max_len (defines\n    of how long the sentence lengths should be).\n    \"\"\"       \n    tokens = tokenizer(texts, max_length=max_len, \n                    truncation=True, padding='max_length',\n                    add_special_tokens=True, return_tensors='np')\n    \n    return tokens","metadata":{"execution":{"iopub.status.busy":"2023-05-02T10:08:46.798563Z","iopub.execute_input":"2023-05-02T10:08:46.799547Z","iopub.status.idle":"2023-05-02T10:08:46.805620Z","shell.execute_reply.started":"2023-05-02T10:08:46.799502Z","shell.execute_reply":"2023-05-02T10:08:46.804690Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Encoding comment_text","metadata":{}},{"cell_type":"code","source":"## Intializing the tokenizer ##\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\ntrain_inputs = encode(train['comment_text'].values.tolist(), \n                      tokenizer, max_len=MAX_LEN)\nvalidation_inputs = encode(valid['comment_text'].values.tolist(),\n                          tokenizer, max_len=MAX_LEN)\ntest_inputs = encode(test['content'].values.tolist(),\n                    tokenizer, max_len=MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T10:08:47.818852Z","iopub.execute_input":"2023-05-02T10:08:47.819290Z","iopub.status.idle":"2023-05-02T10:10:24.014681Z","shell.execute_reply.started":"2023-05-02T10:08:47.819258Z","shell.execute_reply":"2023-05-02T10:10:24.013400Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Downloading (…)lve/main/config.json: 100%|██████████| 616/616 [00:00<00:00, 176kB/s]\nDownloading (…)tencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:00<00:00, 41.7MB/s]\nDownloading (…)/main/tokenizer.json: 100%|██████████| 9.10M/9.10M [00:00<00:00, 83.5MB/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preparing data using tf.data.Data API","metadata":{}},{"cell_type":"code","source":"def map_fn(input_ids, attention_mask, labels=None):\n    if labels is not None:\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}, labels\n    else:\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}","metadata":{"execution":{"iopub.status.busy":"2023-05-02T10:10:30.060098Z","iopub.execute_input":"2023-05-02T10:10:30.060577Z","iopub.status.idle":"2023-05-02T10:10:30.066548Z","shell.execute_reply.started":"2023-05-02T10:10:30.060540Z","shell.execute_reply":"2023-05-02T10:10:30.065608Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs[\"input_ids\"],\n                                                    train_inputs[\"attention_mask\"],\n                                                   train['toxic']))\ntrain_dataset = train_dataset.map(map_fn)\ntrain_dataset = train_dataset.repeat().shuffle(2048).batch(BATCH_SIZE).prefetch(AUTO)\n\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((validation_inputs['input_ids'],\n                                                         validation_inputs['attention_mask'],\n                                                        valid['toxic']))\nvalidation_dataset = validation_dataset.map(map_fn)\nvalidation_dataset = validation_dataset.batch(BATCH_SIZE).prefetch(AUTO)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_inputs['input_ids'],\n                                                  test_inputs['attention_mask']))\ntest_dataset = test_dataset.map(map_fn)\ntest_dataset = test_dataset.batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T10:10:31.118116Z","iopub.execute_input":"2023-05-02T10:10:31.118536Z","iopub.status.idle":"2023-05-02T10:10:32.022952Z","shell.execute_reply.started":"2023-05-02T10:10:31.118503Z","shell.execute_reply":"2023-05-02T10:10:32.021893Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Building the model","metadata":{}},{"cell_type":"code","source":"def build_model(transformer_layer, max_len):\n    \"\"\"\n    Creating the model input layers, output layers,\n    model definition and compilation.\n        \n    Returns: model object after compiling. \n    \"\"\"\n    input_ids = tf.keras.layers.Input(shape=(max_len,), \n                                      dtype=tf.int32, \n                                      name=\"input_ids\")\n    attention_mask = tf.keras.layers.Input(shape=(max_len,), \n                                       dtype=tf.int32, \n                                       name=\"attention_mask\")\n    embeddings = transformer_layer(input_ids, \n                                 attention_mask=attention_mask)[1]\n    x = tf.keras.layers.Dense(1024, activation='relu')(embeddings)\n    y = tf.keras.layers.Dense(1, activation='sigmoid',name='outputs')(x)\n    model = tf.keras.models.Model(inputs=[input_ids, attention_mask],\n                             outputs=y)\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE,\n                                         weight_decay=WEIGHT_DECAY)\n    loss = tf.keras.losses.BinaryCrossentropy()\n    AUC = tf.keras.metrics.AUC()\n    \n    model.compile(loss=loss, metrics=[AUC], optimizer=optimizer)    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-05-02T10:10:33.108630Z","iopub.execute_input":"2023-05-02T10:10:33.108985Z","iopub.status.idle":"2023-05-02T10:10:33.119273Z","shell.execute_reply.started":"2023-05-02T10:10:33.108956Z","shell.execute_reply":"2023-05-02T10:10:33.118270Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Loading model on TPUs","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer,\n                        max_len=MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T10:10:35.218773Z","iopub.execute_input":"2023-05-02T10:10:35.220019Z","iopub.status.idle":"2023-05-02T10:11:52.424002Z","shell.execute_reply.started":"2023-05-02T10:10:35.219969Z","shell.execute_reply":"2023-05-02T10:11:52.422752Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Downloading tf_model.h5: 100%|██████████| 2.24G/2.24G [00:27<00:00, 81.9MB/s]\nAll model checkpoint layers were used when initializing TFXLMRobertaModel.\n\nAll the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-large.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training the model on Only English data for 2 epochs","metadata":{}},{"cell_type":"code","source":"train_steps_per_epoch = train_inputs['input_ids'].shape[0] // BATCH_SIZE\ntrain_history = model.fit(train_dataset,\n                         steps_per_epoch=train_steps_per_epoch,\n                         validation_data=validation_dataset,\n                         epochs=2) ","metadata":{"execution":{"iopub.status.busy":"2023-05-02T10:11:59.819038Z","iopub.execute_input":"2023-05-02T10:11:59.820400Z","iopub.status.idle":"2023-05-02T11:01:53.423654Z","shell.execute_reply.started":"2023-05-02T10:11:59.820330Z","shell.execute_reply":"2023-05-02T11:01:53.422117Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Epoch 1/2\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:459: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n  warnings.warn(\n2023-05-02 10:13:24.704531: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add_790/ReadVariableOp.\n2023-05-02 10:13:27.236463: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add_790/ReadVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"3795/3795 [==============================] - ETA: 0s - loss: 0.0559 - auc: 0.9970","output_type":"stream"},{"name":"stderr","text":"2023-05-02 10:38:13.945567: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n2023-05-02 10:38:14.412802: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"3795/3795 [==============================] - 1601s 375ms/step - loss: 0.0559 - auc: 0.9970 - val_loss: 0.5240 - val_auc: 0.8468\nEpoch 2/2\n3795/3795 [==============================] - 1389s 366ms/step - loss: 0.0466 - auc: 0.9977 - val_loss: 0.3135 - val_auc: 0.8979\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training the model on Validation data for 2 epochs further to fine-tune on it","metadata":{}},{"cell_type":"code","source":"validation_steps_per_epoch = validation_inputs['input_ids'].shape[0] // BATCH_SIZE\nvalidation_history = model.fit(validation_dataset.repeat(),\n                              steps_per_epoch=validation_steps_per_epoch,\n                              epochs=2)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T11:02:00.882510Z","iopub.execute_input":"2023-05-02T11:02:00.883537Z","iopub.status.idle":"2023-05-02T11:04:09.051648Z","shell.execute_reply.started":"2023-05-02T11:02:00.883493Z","shell.execute_reply":"2023-05-02T11:04:09.050299Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch 1/2\n62/62 [==============================] - 22s 362ms/step - loss: 0.2421 - auc: 0.9206\nEpoch 2/2\n62/62 [==============================] - 105s 364ms/step - loss: 0.1640 - auc: 0.9652\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Submit to Competition","metadata":{}},{"cell_type":"code","source":"sub['toxic'] = model.predict(test_dataset, verbose=1)\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-05-02T11:04:39.096307Z","iopub.execute_input":"2023-05-02T11:04:39.096757Z","iopub.status.idle":"2023-05-02T11:05:59.014685Z","shell.execute_reply.started":"2023-05-02T11:04:39.096713Z","shell.execute_reply":"2023-05-02T11:05:59.013143Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"2023-05-02 11:04:48.897770: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n2023-05-02 11:04:49.346644: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n","output_type":"stream"},{"name":"stdout","text":"499/499 [==============================] - 79s 118ms/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- Public Leaderboard Score: 0.9259 and Private Leaderboard Score: 0.9264","metadata":{}}]}