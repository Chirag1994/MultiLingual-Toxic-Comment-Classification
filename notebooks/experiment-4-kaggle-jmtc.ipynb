{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-02T12:02:57.456686Z",
     "iopub.status.busy": "2023-05-02T12:02:57.456022Z",
     "iopub.status.idle": "2023-05-02T12:03:38.688573Z",
     "shell.execute_reply": "2023-05-02T12:03:38.687711Z",
     "shell.execute_reply.started": "2023-05-02T12:02:57.456654Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D0502 12:03:35.365494851   13060 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\n",
      "D0502 12:03:35.365534558   13060 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\n",
      "D0502 12:03:35.365539338   13060 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\n",
      "D0502 12:03:35.365542628   13060 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\n",
      "D0502 12:03:35.365545530   13060 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\n",
      "D0502 12:03:35.365548699   13060 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\n",
      "D0502 12:03:35.365551857   13060 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\n",
      "D0502 12:03:35.365554624   13060 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\n",
      "D0502 12:03:35.365557352   13060 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\n",
      "D0502 12:03:35.365560257   13060 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\n",
      "D0502 12:03:35.365563153   13060 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\n",
      "D0502 12:03:35.365565876   13060 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\n",
      "D0502 12:03:35.365568775   13060 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\n",
      "D0502 12:03:35.365583612   13060 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\n",
      "I0502 12:03:35.365829073   13060 ev_epoll1_linux.cc:122]               grpc epoll fd: 62\n",
      "D0502 12:03:35.365850039   13060 ev_posix.cc:144]                      Using polling engine: epoll1\n",
      "D0502 12:03:35.365873216   13060 dns_resolver_ares.cc:822]             Using ares dns resolver\n",
      "D0502 12:03:35.476140749   13060 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\n",
      "D0502 12:03:35.476158202   13060 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\n",
      "D0502 12:03:35.476163235   13060 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\n",
      "D0502 12:03:35.476166987   13060 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\n",
      "D0502 12:03:35.476170734   13060 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\n",
      "D0502 12:03:35.476174462   13060 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\n",
      "D0502 12:03:35.476183136   13060 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\n",
      "D0502 12:03:35.476203493   13060 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\n",
      "D0502 12:03:35.476251491   13060 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\n",
      "D0502 12:03:35.476273956   13060 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\n",
      "D0502 12:03:35.476278520   13060 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\n",
      "D0502 12:03:35.476282591   13060 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\n",
      "D0502 12:03:35.476286961   13060 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\n",
      "D0502 12:03:35.476291340   13060 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\n",
      "D0502 12:03:35.476295665   13060 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\n",
      "D0502 12:03:35.476301091   13060 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\n",
      "I0502 12:03:35.479267139   13060 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\n",
      "I0502 12:03:35.495217002   13060 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\n",
      "E0502 12:03:35.502354811   13060 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2023-05-02T12:03:35.502332594+00:00\"}\n",
      "/usr/local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install transformers --quiet\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import os, gc\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel\n",
    "from transformers import AutoTokenizer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T12:03:48.960491Z",
     "iopub.status.busy": "2023-05-02T12:03:48.959861Z",
     "iopub.status.idle": "2023-05-02T12:03:48.966023Z",
     "shell.execute_reply": "2023-05-02T12:03:48.965065Z",
     "shell.execute_reply.started": "2023-05-02T12:03:48.960460Z"
    }
   },
   "outputs": [],
   "source": [
    "main_data_dir_path = \"../input/jigsaw-multilingual-toxic-comment-classification/\"\n",
    "toxic_comment_train_csv_path = main_data_dir_path + \"jigsaw-toxic-comment-train.csv\"\n",
    "unintended_bias_train_csv_path = main_data_dir_path + \"jigsaw-unintended-bias-train.csv\"\n",
    "validation_csv_path = main_data_dir_path + \"validation.csv\"\n",
    "test_csv_path = main_data_dir_path + \"test.csv\"\n",
    "submission_csv_path = main_data_dir_path + \"sample_submission.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPU Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-02T12:03:51.932177Z",
     "iopub.status.busy": "2023-05-02T12:03:51.931295Z",
     "iopub.status.idle": "2023-05-02T12:04:01.378875Z",
     "shell.execute_reply": "2023-05-02T12:04:01.377733Z",
     "shell.execute_reply.started": "2023-05-02T12:03:51.932141Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  \n",
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n",
      "INFO:tensorflow:Initializing the TPU system: local\n",
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "#################### TPU Configurations ####################\n",
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "# Configuration\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "MAX_LEN = 192\n",
    "MODEL = 'xlm-roberta-large'\n",
    "NUM_SAMPLES = 150000\n",
    "RANDOM_STATE = 42\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading & Balancing the data by Target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T12:04:04.870538Z",
     "iopub.status.busy": "2023-05-02T12:04:04.869680Z",
     "iopub.status.idle": "2023-05-02T12:04:23.311100Z",
     "shell.execute_reply": "2023-05-02T12:04:23.309666Z",
     "shell.execute_reply.started": "2023-05-02T12:04:04.870499Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reading csv files \n",
    "train1 = pd.read_csv(toxic_comment_train_csv_path)\n",
    "train2 = pd.read_csv(unintended_bias_train_csv_path)\n",
    "valid = pd.read_csv(validation_csv_path)\n",
    "test = pd.read_csv(test_csv_path)\n",
    "sub = pd.read_csv(submission_csv_path)\n",
    "\n",
    "## Converting floating points to integers ##\n",
    "train2.toxic = train2['toxic'].round().astype(int)\n",
    "\n",
    "##### BALANCING THE DATA ##### \n",
    "# : Taking all the data from toxic_comment_train_file & all data corresponding to unintended bias train file\n",
    "# & sampling 150k observations randomly from non-toxic observation population.\n",
    "\n",
    "# Combine train1 with a subset of train2\n",
    "train = pd.concat([\n",
    "    train1[['comment_text', 'toxic']],\n",
    "    train2[['comment_text', 'toxic']].query('toxic==1'),\n",
    "    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=NUM_SAMPLES, random_state=RANDOM_STATE)\n",
    "])\n",
    "\n",
    "## Dropping missing observations with respect to comment-text column \n",
    "train = train.dropna(subset=['comment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T12:04:25.974402Z",
     "iopub.status.busy": "2023-05-02T12:04:25.973964Z",
     "iopub.status.idle": "2023-05-02T12:04:26.002776Z",
     "shell.execute_reply": "2023-05-02T12:04:26.001328Z",
     "shell.execute_reply.started": "2023-05-02T12:04:25.974369Z"
    }
   },
   "outputs": [],
   "source": [
    "## Removing URL's\n",
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    Function takes a sentence and removes the URLs from the provided sentence.\n",
    "    \"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "# define a regular expression pattern to match non-alphanumeric characters\n",
    "pattern = r\"[^a-zA-Z0-9 ]\"\n",
    "\n",
    "# define a function to remove non-alphanumeric characters from a string using the pattern\n",
    "def remove_non_alphanumeric(text):\n",
    "    return re.sub(pattern, \"\", text)\n",
    "\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text: str, punctuations: str):\n",
    "    \"\"\"\n",
    "    Function takes a sentence & a punctuations string to remove punctuations from a sentence.\n",
    "    \"\"\"\n",
    "    return text.translate(str.maketrans(\"\", \"\", punctuations))\n",
    "\n",
    "## Removing multi-characters\n",
    "def remove_multiplechars(text):\n",
    "    text = re.sub(r'(.)\\1{3,}',r'\\1', text)\n",
    "    return text\n",
    "\n",
    "def contraction_to_expansion(text: str, contractions_dict: dict):\n",
    "    \"\"\"\n",
    "    Function takes a sentence (text) and a dictionary to map words like ain't to am not etc.\n",
    "    and returns the final sentence.\n",
    "    \"\"\"\n",
    "    if type(text) is str:\n",
    "        for key in contractions_dict:\n",
    "            value = contractions_dict[key]\n",
    "            text = text.replace(key, value)\n",
    "        return text\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "contractions = {\"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
    "                \"'cause\": \"because\",\"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                \"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\n",
    "                \"hadn't\": \"had not\", \"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\n",
    "                \"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
    "                \"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\n",
    "                \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\n",
    "                \"I've\": \"I have\",\"isn't\": \"is not\",\"it'd\": \"it had\", \"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
    "                \"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n",
    "                \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\n",
    "                \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
    "                \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\", \n",
    "                \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "                \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\", \"she'll\": \"she will\",\n",
    "                \"she'll've\": \"she will have\",\"she's\": \"she is\",\"should've\": \"should have\",\"shouldn't\": \"should not\",\n",
    "                \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\",\n",
    "                \"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there had\",\n",
    "                \"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\n",
    "                \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\n",
    "                \"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\",\n",
    "                \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\n",
    "                \"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
    "                \"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\n",
    "                \"when've\": \"when have\", \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\n",
    "                \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                \"why's\": \"why is\",\"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
    "                \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
    "                \"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\n",
    "                \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                \"you'd\": \"you had\", \"you'd've\": \"you would have\",\"you'll\": \"you you will\",\n",
    "                \"you'll've\": \"you you will have\",\"you're\": \"you are\",\"you've\": \"you have\"}\n",
    "\n",
    "#################################################### CLEANING THE DATA ####################################################\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\n\", \"\") ## Remove the next line character ##\n",
    "    text = remove_urls(text) ## Remove the URLs ##\n",
    "    text = re.compile(r'ï¼¼\\(.+?\\)/').sub(\"\", text) ## Remove emoticons ##\n",
    "    text = remove_non_alphanumeric(text) ## Remove Unicode characters ##\n",
    "    text = remove_punctuation(text, PUNCT_TO_REMOVE) ## Removing Punctuations ##\n",
    "    text = remove_multiplechars(text) ## Removing multiple characters in a word ##\n",
    "    text = contraction_to_expansion(text, contractions)\n",
    "    return text\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T12:04:29.799661Z",
     "iopub.status.busy": "2023-05-02T12:04:29.799195Z",
     "iopub.status.idle": "2023-05-02T12:05:45.656552Z",
     "shell.execute_reply": "2023-05-02T12:05:45.655098Z",
     "shell.execute_reply.started": "2023-05-02T12:04:29.799627Z"
    }
   },
   "outputs": [],
   "source": [
    "###### CLEANING THE DATA ######\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(lambda x: clean_text(x))\n",
    "valid['comment_text'] = valid['comment_text'].apply(lambda x: clean_text(x))\n",
    "test['content'] = test['content'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T12:05:45.658847Z",
     "iopub.status.busy": "2023-05-02T12:05:45.658520Z",
     "iopub.status.idle": "2023-05-02T12:05:45.664755Z",
     "shell.execute_reply": "2023-05-02T12:05:45.663778Z",
     "shell.execute_reply.started": "2023-05-02T12:05:45.658821Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode(texts, tokenizer, max_len):\n",
    "    \"\"\"\n",
    "    Function takes a list of texts, tokenizer (object)\n",
    "    initialized from HuggingFace library, max_len (defines\n",
    "    of how long the sentence lengths should be).\n",
    "    \"\"\"       \n",
    "    tokens = tokenizer(texts, max_length=max_len, \n",
    "                    truncation=True, padding='max_length',\n",
    "                    add_special_tokens=True, return_tensors='np')\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T12:05:50.172830Z",
     "iopub.status.busy": "2023-05-02T12:05:50.171725Z",
     "iopub.status.idle": "2023-05-02T12:06:51.165204Z",
     "shell.execute_reply": "2023-05-02T12:06:51.163727Z",
     "shell.execute_reply.started": "2023-05-02T12:05:50.172791Z"
    }
   },
   "outputs": [],
   "source": [
    "## Intializing the tokenizer ##\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "train_inputs = encode(train['comment_text'].values.tolist(), \n",
    "                      tokenizer, max_len=MAX_LEN)\n",
    "validation_inputs = encode(valid['comment_text'].values.tolist(),\n",
    "                          tokenizer, max_len=MAX_LEN)\n",
    "test_inputs = encode(test['content'].values.tolist(),\n",
    "                    tokenizer, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data using tf.data.Data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T12:06:53.812166Z",
     "iopub.status.busy": "2023-05-02T12:06:53.811676Z",
     "iopub.status.idle": "2023-05-02T12:06:53.818586Z",
     "shell.execute_reply": "2023-05-02T12:06:53.817371Z",
     "shell.execute_reply.started": "2023-05-02T12:06:53.812131Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_fn(input_ids, attention_mask, labels=None):\n",
    "    if labels is not None:\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}, labels\n",
    "    else:\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T12:06:55.803699Z",
     "iopub.status.busy": "2023-05-02T12:06:55.803247Z",
     "iopub.status.idle": "2023-05-02T12:06:56.639395Z",
     "shell.execute_reply": "2023-05-02T12:06:56.638030Z",
     "shell.execute_reply.started": "2023-05-02T12:06:55.803671Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs[\"input_ids\"],\n",
    "                                                    train_inputs[\"attention_mask\"],\n",
    "                                                   train['toxic']))\n",
    "train_dataset = train_dataset.map(map_fn)\n",
    "train_dataset = train_dataset.repeat().shuffle(2048).batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_inputs['input_ids'],\n",
    "                                                         validation_inputs['attention_mask'],\n",
    "                                                        valid['toxic']))\n",
    "validation_dataset = validation_dataset.map(map_fn)\n",
    "validation_dataset = validation_dataset.batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_inputs['input_ids'],\n",
    "                                                  test_inputs['attention_mask']))\n",
    "test_dataset = test_dataset.map(map_fn)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T12:07:00.234262Z",
     "iopub.status.busy": "2023-05-02T12:07:00.233807Z",
     "iopub.status.idle": "2023-05-02T12:07:00.245415Z",
     "shell.execute_reply": "2023-05-02T12:07:00.244196Z",
     "shell.execute_reply.started": "2023-05-02T12:07:00.234231Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(transformer_layer, max_len):\n",
    "    \"\"\"\n",
    "    Creating the model input layers, output layers,\n",
    "    model definition and compilation.\n",
    "        \n",
    "    Returns: model object after compiling. \n",
    "    \"\"\"\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), \n",
    "                                      dtype=tf.int32, \n",
    "                                      name=\"input_ids\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_len,), \n",
    "                                       dtype=tf.int32, \n",
    "                                       name=\"attention_mask\")\n",
    "    embeddings = transformer_layer(input_ids, \n",
    "                                 attention_mask=attention_mask)[1]\n",
    "    x = tf.keras.layers.Dense(1024, activation='relu')(embeddings)\n",
    "    y = tf.keras.layers.Dense(1, activation='sigmoid',name='outputs')(x)\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask],\n",
    "                             outputs=y)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE,\n",
    "                                         weight_decay=WEIGHT_DECAY)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    AUC = tf.keras.metrics.AUC()\n",
    "    \n",
    "    model.compile(loss=loss, metrics=[AUC], optimizer=optimizer)    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model on TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-02T12:07:03.043385Z",
     "iopub.status.busy": "2023-05-02T12:07:03.042913Z",
     "iopub.status.idle": "2023-05-02T12:07:53.709787Z",
     "shell.execute_reply": "2023-05-02T12:07:53.708323Z",
     "shell.execute_reply.started": "2023-05-02T12:07:03.043351Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFXLMRobertaModel.\n",
      "\n",
      "All the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
    "    model = build_model(transformer_layer,\n",
    "                        max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model on Only English data for 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T12:07:57.655360Z",
     "iopub.status.busy": "2023-05-02T12:07:57.654920Z",
     "iopub.status.idle": "2023-05-02T12:58:25.467268Z",
     "shell.execute_reply": "2023-05-02T12:58:25.465776Z",
     "shell.execute_reply.started": "2023-05-02T12:07:57.655327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:459: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "2023-05-02 12:09:24.265620: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add_790/ReadVariableOp.\n",
      "2023-05-02 12:09:26.808414: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add_790/ReadVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3795/3795 [==============================] - ETA: 0s - loss: 0.0566 - auc: 0.9968"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 12:34:30.082733: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n",
      "2023-05-02 12:34:30.666205: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3795/3795 [==============================] - 1621s 380ms/step - loss: 0.0566 - auc: 0.9968 - val_loss: 0.4182 - val_auc: 0.8730\n",
      "Epoch 2/2\n",
      "3795/3795 [==============================] - 1404s 370ms/step - loss: 0.0463 - auc: 0.9978 - val_loss: 0.4332 - val_auc: 0.8800\n"
     ]
    }
   ],
   "source": [
    "train_steps_per_epoch = train_inputs['input_ids'].shape[0] // BATCH_SIZE\n",
    "train_history = model.fit(train_dataset,\n",
    "                         steps_per_epoch=train_steps_per_epoch,\n",
    "                         validation_data=validation_dataset,\n",
    "                         epochs=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model on Validation data for 2 epochs further to fine-tune on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T12:58:30.519887Z",
     "iopub.status.busy": "2023-05-02T12:58:30.519394Z",
     "iopub.status.idle": "2023-05-02T13:00:46.934589Z",
     "shell.execute_reply": "2023-05-02T13:00:46.933120Z",
     "shell.execute_reply.started": "2023-05-02T12:58:30.519850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "62/62 [==============================] - 23s 367ms/step - loss: 0.2702 - auc: 0.8978\n",
      "Epoch 2/2\n",
      "62/62 [==============================] - 113s 367ms/step - loss: 0.1719 - auc: 0.9612\n"
     ]
    }
   ],
   "source": [
    "validation_steps_per_epoch = validation_inputs['input_ids'].shape[0] // BATCH_SIZE\n",
    "validation_history = model.fit(validation_dataset.repeat(),\n",
    "                              steps_per_epoch=validation_steps_per_epoch,\n",
    "                              epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-02T13:00:50.283451Z",
     "iopub.status.busy": "2023-05-02T13:00:50.282213Z",
     "iopub.status.idle": "2023-05-02T13:02:11.446608Z",
     "shell.execute_reply": "2023-05-02T13:02:11.445019Z",
     "shell.execute_reply.started": "2023-05-02T13:00:50.283407Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 13:00:59.514371: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2023-05-02 13:01:00.008000: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499/499 [==============================] - 80s 120ms/step\n"
     ]
    }
   ],
   "source": [
    "sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Public Leaderboard Score: 0.8755 and Private Leaderboard Score: 0.8754"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
