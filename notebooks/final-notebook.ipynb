{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "The Conversation AI team, a research initiative founded by Jigsaw and Google builds a technology to prevent voices in Conversation. In 2020, Jigsaw organized a competition on Kaggle where the competitors has to build machine learning models that can identify toxicity in Online conversations, where toxicity is defined as `anything rude, disrespectful, or otherwise likely` to make someone leave the discussion. If these contributions can be identified, we could have a safer, more collaborative internet.     \n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "As part of the competition, competitors were provided several files, specifically:\n",
    "\n",
    "`jigsaw-toxic-comment-train.csv` - data from the [Jigsaw toxic comment classification competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). The dataset is made up of English comments from Wikipediaâ€™s talk page edits.\n",
    "\n",
    "`jigsaw-unintended-bias-train.csv` - data from the [Jigsaw Unintended Bias in Toxicity Classification competition](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification). This is an expanded version of the Civil Comments dataset with a range of additional labels.\n",
    "\n",
    "`sample_submission.csv` - a sample submission file.\n",
    "\n",
    "`test.csv` - comments from Wikipedia talk pages in different non-English languages.\n",
    "\n",
    "`validation.csv` - comments from Wikipedia talk pages in different non-English languages\n",
    "\n",
    "## Evaluation Metric\n",
    "Submissions were evaluated based on Area Under the ROC Curve between the predicted probability and the observed target.\n",
    "\n",
    "## Strategies to Tackle\n",
    "### `Monolingual Approach`\n",
    "Monolingual models are the type of language models which are trained on a single language.\n",
    "\n",
    "They are focused on understanding and generating text in a specific language. \n",
    "\n",
    "For example, a monolingual model trained on English language will be proficient in understanding and generating English text. \n",
    "\n",
    "These models are typically for tasks such as text classification, sentiment analysis and more within a specific language.\n",
    "\n",
    "Monolingual models can be beneficial to utilize when we have a specific language in our training, testing datasets and in the upcoming unseen data. \n",
    "\n",
    "### `Multilingual Approach`\n",
    "Multilingual models are, on the other hand, are trained on multiple different languages. \n",
    "\n",
    "They are designed to handle and process text in multiple languages, allowing them to perform across different languages. \n",
    "\n",
    "Multilingual models have the advantage of of being able to provide language-agnostic solutions, as they can handle a wide-range of languages. \n",
    "\n",
    "They can be used for zero-shot and few-shot learning, where the model can perform a task in a language it has not been seen specifically trained on by leveraging its knowledge of other languages.\n",
    "\n",
    "### `Which models to use for our problem?`\n",
    "As per the dataset given in the competition, we have only english data in our training dataset and very samples are given in the validation dataset containing languages `Spanish`, `Turkish` and `Italian` only and the Testing dataset contains languages `Turkish`, `Spanish`, `Italian`, `Russian`, `French` and `Portugese`.\n",
    "\n",
    "Since in our validation and test dataset contains non-english languages it would be better approach to build multilingual models rather than monolingual models. \n",
    "\n",
    "Now, if we had only one language (as stated above) building monolingual models would be a better choice. \n",
    "\n",
    "Let's discuss Multilingual models approach a bit more:\n",
    "\n",
    "How are multilingual models are trained?\n",
    "\n",
    "Multilingual models are pre-trained on a mix of different languages and they don't distinguish between the languages. \n",
    "\n",
    "The English BERT was pre-trained on English Wikipedia and BookCorpus dataset, while multilingual models like mBERT was pre-trained on 102 different languages from largest Wikipedia dataset and XLM-Roberta was pre-trained on CommonCrawl dataset from 100 different languages respectively.\n",
    "\n",
    "### `Cross Lingual Transfer`\n",
    "Cross-lingual transfer refers to transfer learning using data and models available for one language for which ample such resources are available (e.g., English) to solve tasks in another, commonly more low-resource, language.\n",
    "\n",
    "In our case, we are trying to create an application that can automatically detect whether a sentence or phrase is toxic or not. \n",
    "\n",
    "Models like XLM-Roberta provides us the ability to fine tune it on English dataset and predict to identify comments in any different language.\n",
    "\n",
    "XLM-R is able to take it's specific knowledge learnt in one language and apply it to a different langauge (languages), even though it never seen the language during fine-tuning. \n",
    "\n",
    "This concept is of transfer learning applied from one language to another language is referred to as `Cross-Lingual Transfer (AKA Zero-Shot Learning)` .\n",
    "\n",
    "Another reason to use Pre-Trained multi-lingual models for a task like this (as in our case) is that is the `Lack of languages by resources` i.e., different languages have different amounts of training data available to build models like BERT and its variants. \n",
    "\n",
    "Some languages like English, Chinese, Russian, Indonesian, Vietnamese etc. are the languages that have high resource languages, whereas languages like sundanese, assamese etc. are low resource languages. \n",
    "\n",
    "Training our own BERT like model on these low resources could be very expensive in terms of data collection and performance-wise  , therefore, We should leverage these multi-lingual models.\n",
    "\n",
    "### `What experiments did I perform ?`\n",
    "At the Overall level, I performed 9 experiments with the following ideas keeping in mind.\n",
    "\n",
    "Perform Pre-processing techniques like removal of stopwords, removing URLs, Contraction to Expansion of words, removing multiple characters from words and removal of punctuations.\n",
    "\n",
    "From model stand point we experimented with 2 models mBERT & XLM-Roberta.\n",
    "\n",
    "From Training dataset perspective we used 2 types of datasets: one case where we used the provided training datasets where we tried to balance the dataset by the target and the other case where we used the translated training dataset of languages provided in the test dataset along with the english language with class balancing.\n",
    "\n",
    "We always trained on validation dataset as well to further boost the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a model with the following ideas:\n",
    "> Training on original training & validation datasets, class balancing (undersampling), fine-tuning the model for 2 epochs on training as well as validation dataset, and will not perform any preprocessing dataset. We will be leveraging the TPUs offered by Kaggle.\n",
    "\n",
    "## Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-06T05:33:29.291381Z",
     "iopub.status.busy": "2023-05-06T05:33:29.290778Z",
     "iopub.status.idle": "2023-05-06T05:34:10.087774Z",
     "shell.execute_reply": "2023-05-06T05:34:10.086634Z",
     "shell.execute_reply.started": "2023-05-06T05:33:29.291332Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/site-packages (from nltk) (2023.5.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D0506 05:34:07.119779858    4677 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\n",
      "D0506 05:34:07.119807585    4677 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\n",
      "D0506 05:34:07.119811295    4677 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\n",
      "D0506 05:34:07.119814245    4677 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\n",
      "D0506 05:34:07.119816782    4677 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\n",
      "D0506 05:34:07.119819332    4677 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\n",
      "D0506 05:34:07.119822001    4677 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\n",
      "D0506 05:34:07.119824463    4677 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\n",
      "D0506 05:34:07.119827037    4677 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\n",
      "D0506 05:34:07.119829546    4677 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\n",
      "D0506 05:34:07.119831992    4677 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\n",
      "D0506 05:34:07.119834486    4677 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\n",
      "D0506 05:34:07.119837139    4677 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\n",
      "D0506 05:34:07.119839615    4677 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\n",
      "I0506 05:34:07.120040133    4677 ev_epoll1_linux.cc:122]               grpc epoll fd: 66\n",
      "D0506 05:34:07.120052838    4677 ev_posix.cc:144]                      Using polling engine: epoll1\n",
      "D0506 05:34:07.120073152    4677 dns_resolver_ares.cc:822]             Using ares dns resolver\n",
      "D0506 05:34:07.120489142    4677 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\n",
      "D0506 05:34:07.120501042    4677 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\n",
      "D0506 05:34:07.120505176    4677 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\n",
      "D0506 05:34:07.120508439    4677 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\n",
      "D0506 05:34:07.120511867    4677 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\n",
      "D0506 05:34:07.120515389    4677 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\n",
      "D0506 05:34:07.120522127    4677 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\n",
      "D0506 05:34:07.120542192    4677 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\n",
      "D0506 05:34:07.120570394    4677 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\n",
      "D0506 05:34:07.120597972    4677 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\n",
      "D0506 05:34:07.120602878    4677 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\n",
      "D0506 05:34:07.120606537    4677 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\n",
      "D0506 05:34:07.120612698    4677 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\n",
      "D0506 05:34:07.120616442    4677 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\n",
      "D0506 05:34:07.120620040    4677 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\n",
      "D0506 05:34:07.120624632    4677 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\n",
      "I0506 05:34:07.123119246    4677 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\n",
      "I0506 05:34:07.139784031    4906 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\n",
      "E0506 05:34:07.146844269    4906 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2023-05-06T05:34:07.146824378+00:00\", grpc_status:2}\n",
      "/usr/local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install transformers --quiet\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import os, gc\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel\n",
    "from transformers import AutoTokenizer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T05:34:12.270386Z",
     "iopub.status.busy": "2023-05-06T05:34:12.269132Z",
     "iopub.status.idle": "2023-05-06T05:34:12.276376Z",
     "shell.execute_reply": "2023-05-06T05:34:12.275234Z",
     "shell.execute_reply.started": "2023-05-06T05:34:12.270346Z"
    }
   },
   "outputs": [],
   "source": [
    "main_data_dir_path = \"../input/jigsaw-multilingual-toxic-comment-classification/\"\n",
    "toxic_comment_train_csv_path = main_data_dir_path + \"jigsaw-toxic-comment-train.csv\"\n",
    "unintended_bias_train_csv_path = main_data_dir_path + \"jigsaw-unintended-bias-train.csv\"\n",
    "validation_csv_path = main_data_dir_path + \"validation.csv\"\n",
    "test_csv_path = main_data_dir_path + \"test.csv\"\n",
    "submission_csv_path = main_data_dir_path + \"sample_submission.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPU Configurations\n",
    "\n",
    "Intializing the TPU configurations and other constants like `number of epochs, batch_size (16 * number of cores offered on TPUS), MAX_LEN (length of the sentence), we use xlm-roberta-large model, number of samples (for undersampling) = 150k, Learning_rate = 1e-5 etc.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-06T05:34:13.990258Z",
     "iopub.status.busy": "2023-05-06T05:34:13.989127Z",
     "iopub.status.idle": "2023-05-06T05:34:24.240364Z",
     "shell.execute_reply": "2023-05-06T05:34:24.239409Z",
     "shell.execute_reply.started": "2023-05-06T05:34:13.990220Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  \n",
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n",
      "INFO:tensorflow:Initializing the TPU system: local\n",
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "#################### TPU Configurations ####################\n",
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "# Configuration\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "MAX_LEN = 192\n",
    "MODEL = 'xlm-roberta-large'\n",
    "NUM_SAMPLES = 150000\n",
    "RANDOM_STATE = 42\n",
    "LEARNING_RATE = 1e-5 ######################### MAIN CHANGE ############################\n",
    "WEIGHT_DECAY = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading & Balancing the data by Target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T05:34:30.240536Z",
     "iopub.status.busy": "2023-05-06T05:34:30.239712Z",
     "iopub.status.idle": "2023-05-06T05:34:48.127552Z",
     "shell.execute_reply": "2023-05-06T05:34:48.126285Z",
     "shell.execute_reply.started": "2023-05-06T05:34:30.240499Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reading csv files \n",
    "train1 = pd.read_csv(toxic_comment_train_csv_path)\n",
    "train2 = pd.read_csv(unintended_bias_train_csv_path)\n",
    "valid = pd.read_csv(validation_csv_path)\n",
    "test = pd.read_csv(test_csv_path)\n",
    "sub = pd.read_csv(submission_csv_path)\n",
    "\n",
    "## Converting floating points to integers ##\n",
    "train2.toxic = train2['toxic'].round().astype(int)\n",
    "\n",
    "##### BALANCING THE DATA ##### \n",
    "# : Taking all the data from toxic_comment_train_file & all data corresponding to unintended bias train file\n",
    "# & sampling 150k observations randomly from non-toxic observation population.\n",
    "\n",
    "# Combine train1 with a subset of train2\n",
    "train = pd.concat([\n",
    "    train1[['comment_text', 'toxic']],\n",
    "    train2[['comment_text', 'toxic']].query('toxic==1'),\n",
    "    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=NUM_SAMPLES, random_state=RANDOM_STATE)\n",
    "])\n",
    "\n",
    "## Dropping missing observations with respect to comment-text column \n",
    "train = train.dropna(subset=['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T05:34:52.659521Z",
     "iopub.status.busy": "2023-05-06T05:34:52.658685Z",
     "iopub.status.idle": "2023-05-06T05:34:52.664993Z",
     "shell.execute_reply": "2023-05-06T05:34:52.664079Z",
     "shell.execute_reply.started": "2023-05-06T05:34:52.659472Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode(texts, tokenizer, max_len):\n",
    "    \"\"\"\n",
    "    Function takes a list of texts, tokenizer (object)\n",
    "    initialized from HuggingFace library, max_len (defines\n",
    "    of how long the sentence lengths should be).\n",
    "    \"\"\"       \n",
    "    tokens = tokenizer(texts, max_length=max_len, \n",
    "                    truncation=True, padding='max_length',\n",
    "                    add_special_tokens=True, return_tensors='np')\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding comment_text\n",
    "\n",
    "We first initialize the tokenizer from Hugging Face transformer library and encoding our training, validation and test dataset comment_texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T05:34:53.719791Z",
     "iopub.status.busy": "2023-05-06T05:34:53.718895Z",
     "iopub.status.idle": "2023-05-06T05:35:56.623240Z",
     "shell.execute_reply": "2023-05-06T05:35:56.621611Z",
     "shell.execute_reply.started": "2023-05-06T05:34:53.719755Z"
    }
   },
   "outputs": [],
   "source": [
    "## Intializing the tokenizer ##\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "train_inputs = encode(train['comment_text'].values.tolist(), \n",
    "                      tokenizer, max_len=MAX_LEN)\n",
    "validation_inputs = encode(valid['comment_text'].values.tolist(),\n",
    "                          tokenizer, max_len=MAX_LEN)\n",
    "test_inputs = encode(test['content'].values.tolist(),\n",
    "                    tokenizer, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data using tf.data.Data API\n",
    "\n",
    "Writing a function to create a tuple of inputs and outputs, where inputs have a dictionary datatype.\n",
    "\n",
    "We'll be leveraging tf.data.Data API to pass our inputs and outputs as tuple, i.e., (inputs, outputs), inputs are `{\"input_ids\": input_ids, \"attention_mask\": attention_mask} and outputs labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T05:36:01.089870Z",
     "iopub.status.busy": "2023-05-06T05:36:01.089427Z",
     "iopub.status.idle": "2023-05-06T05:36:01.095972Z",
     "shell.execute_reply": "2023-05-06T05:36:01.094782Z",
     "shell.execute_reply.started": "2023-05-06T05:36:01.089839Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_fn(input_ids, attention_mask, labels=None):\n",
    "    if labels is not None:\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}, labels\n",
    "    else:\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T05:36:01.990119Z",
     "iopub.status.busy": "2023-05-06T05:36:01.989015Z",
     "iopub.status.idle": "2023-05-06T05:36:02.856194Z",
     "shell.execute_reply": "2023-05-06T05:36:02.854949Z",
     "shell.execute_reply.started": "2023-05-06T05:36:01.990082Z"
    }
   },
   "outputs": [],
   "source": [
    "#### Creating the training dataset ####\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs[\"input_ids\"],\n",
    "                                                    train_inputs[\"attention_mask\"],\n",
    "                                                   train['toxic']))\n",
    "train_dataset = train_dataset.map(map_fn)\n",
    "train_dataset = train_dataset.repeat().shuffle(buffer_size=2048,seed=RANDOM_STATE).batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "#### Creating the validation dataset ####\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_inputs['input_ids'],\n",
    "                                                         validation_inputs['attention_mask'],\n",
    "                                                        valid['toxic']))\n",
    "validation_dataset = validation_dataset.map(map_fn)\n",
    "validation_dataset = validation_dataset.batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "#### Creating the testing dataset ####\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_inputs['input_ids'],\n",
    "                                                  test_inputs['attention_mask']))\n",
    "test_dataset = test_dataset.map(map_fn)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T05:36:04.650085Z",
     "iopub.status.busy": "2023-05-06T05:36:04.649666Z",
     "iopub.status.idle": "2023-05-06T05:36:04.661209Z",
     "shell.execute_reply": "2023-05-06T05:36:04.660114Z",
     "shell.execute_reply.started": "2023-05-06T05:36:04.650055Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(transformer_layer, max_len):\n",
    "    \"\"\"\n",
    "    Creating the model input layers, output layers,\n",
    "    model definition and compilation.\n",
    "        \n",
    "    Returns: model object after compiling. \n",
    "    \"\"\"\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_len,), \n",
    "                                      dtype=tf.int32, \n",
    "                                      name=\"input_ids\")\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_len,), \n",
    "                                       dtype=tf.int32, \n",
    "                                       name=\"attention_mask\")\n",
    "    output = transformer_layer.roberta(input_ids, ############# NOTE THAT this .roberta property is very important to use.\n",
    "                                       ###### otherwise we'll get a nasty error while loading our saved model.   \n",
    "                                 attention_mask=attention_mask)[1]\n",
    "    x = tf.keras.layers.Dense(1024, activation='relu')(output)\n",
    "    y = tf.keras.layers.Dense(1, activation='sigmoid',name='outputs')(x)\n",
    "    model = tf.keras.models.Model(inputs=[input_ids, attention_mask],\n",
    "                             outputs=y)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE,\n",
    "                                         weight_decay=WEIGHT_DECAY)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    AUC = tf.keras.metrics.AUC()\n",
    "    \n",
    "    model.compile(loss=loss, metrics=[AUC], optimizer=optimizer)    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model on TPUs\n",
    "\n",
    "It is important to initialize & compile the model inside the `with strategy.scope()`.\n",
    "\n",
    "One thing I want to point out that for some reason I was getting different results even though I was setting the seed before initializing the model, but the results are always consistent even though the results differ very little every time we run the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-06T05:36:07.379241Z",
     "iopub.status.busy": "2023-05-06T05:36:07.378830Z",
     "iopub.status.idle": "2023-05-06T05:36:57.043546Z",
     "shell.execute_reply": "2023-05-06T05:36:57.042138Z",
     "shell.execute_reply.started": "2023-05-06T05:36:07.379210Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFXLMRobertaModel.\n",
      "\n",
      "All the layers of TFXLMRobertaModel were initialized from the model checkpoint at xlm-roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 192)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 192)]        0           []                               \n",
      "                                                                                                  \n",
      " roberta (TFXLMRobertaMainLayer  TFBaseModelOutputWi  559890432  ['input_ids[0][0]',              \n",
      " )                              thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 192,                                               \n",
      "                                 1024),                                                           \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 1024),                                                         \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         1049600     ['roberta[0][1]']                \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 1)            1025        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 560,941,057\n",
      "Trainable params: 560,941,057\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
    "    tf.random.set_seed(RANDOM_STATE)\n",
    "    model = build_model(transformer_layer,\n",
    "                        max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model on Only English data for 2 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-06T05:37:05.852249Z",
     "iopub.status.busy": "2023-05-06T05:37:05.851019Z",
     "iopub.status.idle": "2023-05-06T06:27:33.868007Z",
     "shell.execute_reply": "2023-05-06T06:27:33.866630Z",
     "shell.execute_reply.started": "2023-05-06T05:37:05.852208Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:459: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "2023-05-06 05:38:32.900327: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add_790/ReadVariableOp.\n",
      "2023-05-06 05:38:35.393659: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add_790/ReadVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3795/3795 [==============================] - ETA: 0s - loss: 0.0547 - auc: 0.9970"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 06:03:39.886988: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n",
      "2023-05-06 06:03:40.421135: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Add/ReadVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3795/3795 [==============================] - 1623s 379ms/step - loss: 0.0547 - auc: 0.9970 - val_loss: 0.3265 - val_auc: 0.9095\n",
      "Epoch 2/2\n",
      "3795/3795 [==============================] - 1402s 369ms/step - loss: 0.0448 - auc: 0.9980 - val_loss: 0.3493 - val_auc: 0.8918\n"
     ]
    }
   ],
   "source": [
    "train_steps_per_epoch = train_inputs['input_ids'].shape[0] // BATCH_SIZE\n",
    "train_history = model.fit(train_dataset,\n",
    "                         steps_per_epoch=train_steps_per_epoch,\n",
    "                         validation_data=validation_dataset,\n",
    "                         epochs=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model on Validation data for 2 epochs further to fine-tune on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-06T06:27:52.849722Z",
     "iopub.status.busy": "2023-05-06T06:27:52.848713Z",
     "iopub.status.idle": "2023-05-06T06:30:12.417632Z",
     "shell.execute_reply": "2023-05-06T06:30:12.416112Z",
     "shell.execute_reply.started": "2023-05-06T06:27:52.849680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "62/62 [==============================] - 23s 367ms/step - loss: 0.2313 - auc: 0.9276\n",
      "Epoch 2/2\n",
      "62/62 [==============================] - 116s 367ms/step - loss: 0.1499 - auc: 0.9719\n"
     ]
    }
   ],
   "source": [
    "validation_steps_per_epoch = validation_inputs['input_ids'].shape[0] // BATCH_SIZE\n",
    "validation_history = model.fit(validation_dataset.repeat(),\n",
    "                              steps_per_epoch=validation_steps_per_epoch,\n",
    "                              epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-06T06:30:15.829199Z",
     "iopub.status.busy": "2023-05-06T06:30:15.828019Z",
     "iopub.status.idle": "2023-05-06T06:31:36.539433Z",
     "shell.execute_reply": "2023-05-06T06:31:36.538050Z",
     "shell.execute_reply.started": "2023-05-06T06:30:15.829165Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 06:30:25.204263: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2023-05-06 06:30:25.715181: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499/499 [==============================] - 80s 119ms/step\n"
     ]
    }
   ],
   "source": [
    "sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-06T06:31:36.541448Z",
     "iopub.status.busy": "2023-05-06T06:31:36.541022Z",
     "iopub.status.idle": "2023-05-06T06:31:36.552962Z",
     "shell.execute_reply": "2023-05-06T06:31:36.551863Z",
     "shell.execute_reply.started": "2023-05-06T06:31:36.541418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.157186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     toxic\n",
       "0   0  0.000168\n",
       "1   1  0.000202\n",
       "2   2  0.157186\n",
       "3   3  0.000174\n",
       "4   4  0.000155"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Public LeaderBoard score on kaggle (test dataset): 0.936 and Private LeaderBoard score : 0.9346\n",
    "\n",
    "## Results\n",
    "| Experiment | Public Test LeaderBoard Score | Private Test LeaderBoard Score |\n",
    "| --- | --- | --- |\n",
    "| 1 (mBERT + No Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 2e-5) | 0.8850 | 0.8869 |\n",
    "|2 (xlm-roberta-large + No Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 2e-5) | 0.9259 | 0.9264 |\n",
    "|3 (mBERT + Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 2e-5) | 0.8259 | 0.8239 |\n",
    "|4 (xlm-roberta-large + Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 2e-5) | 0.8755 | 0.8754 |\n",
    "|5 (mBERT + No Preprocessing + BCE Loss + Fine tune on translated in languages present in test (along with english original english) training and validation datasets for 2 epochs each + Learning_rate = 1e-5) |  0.9195 | 0.9212 |\n",
    "|6 ((xlm-roberta-large + No Preprocessing + BCE Loss + Fine tune on translated in languages present in test (along with english original english) training and validation datasets for 2 epochs each + Learning_rate = 1e-5) |  0.9329 | 0.9212 |\n",
    "|7 (mBERT + Preprocessing + BCE Loss + Fine tune on translated in languages present in test (along with english original english) training and validation datasets for 2 epochs each + Learning_rate = 1e-5) |  0.8696 | 0.9212 |\n",
    "|8 ((xlm-roberta-large + Preprocessing + BCE Loss + Fine tune on translated in languages present in test (along with english original english) training and validation datasets for 2 epochs each + Learning_rate = 1e-5) |  0.8861 | 0.8866 |\n",
    "|9 (xlm-roberta-large + No Preprocessing + BCE Loss + Fine tune on original training and validation datasets for 2 epochs each + Learning_rate = 1e-5) | 0.936 | 0.9346 |\n",
    "\n",
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-06T06:31:38.334122Z",
     "iopub.status.busy": "2023-05-06T06:31:38.333306Z",
     "iopub.status.idle": "2023-05-06T06:33:49.028122Z",
     "shell.execute_reply": "2023-05-06T06:33:49.026551Z",
     "shell.execute_reply.started": "2023-05-06T06:31:38.334085Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn, pooler_layer_call_and_return_conditional_losses while saving (showing 5 of 829). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../working/Multilingual_toxic_comment_classifier/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../working/Multilingual_toxic_comment_classifier/assets\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"../working/Multilingual_toxic_comment_classifier\"\n",
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T06:33:55.889270Z",
     "iopub.status.busy": "2023-05-06T06:33:55.888848Z",
     "iopub.status.idle": "2023-05-06T06:34:28.902247Z",
     "shell.execute_reply": "2023-05-06T06:34:28.900744Z",
     "shell.execute_reply.started": "2023-05-06T06:33:55.889240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7fb4b1920f70>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model(model_save_path)\n",
    "y = loaded_model.predict(test_dataset.take(1))\n",
    "y[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the function to prepare for the new text, we encode the text using the `tokenizer with the sentence length=192`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T07:15:15.890984Z",
     "iopub.status.busy": "2023-05-06T07:15:15.890629Z",
     "iopub.status.idle": "2023-05-06T07:15:16.946294Z",
     "shell.execute_reply": "2023-05-06T07:15:16.944822Z",
     "shell.execute_reply.started": "2023-05-06T07:15:15.890957Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "\n",
    "text = \"politicians are like cancer for this country\"\n",
    "def prep_data(text, tokenizer, max_len=192):\n",
    "    tokens = tokenizer(text, max_length=max_len, \n",
    "                    truncation=True, padding='max_length',\n",
    "                    add_special_tokens=True, return_tensors='tf')\n",
    "    \n",
    "    return {\"input_ids\": tokens['input_ids'],\n",
    "            \"attention_mask\": tokens['attention_mask']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the probability of toxic and non-toxic on a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T07:16:44.751314Z",
     "iopub.status.busy": "2023-05-06T07:16:44.750562Z",
     "iopub.status.idle": "2023-05-06T07:16:45.340773Z",
     "shell.execute_reply": "2023-05-06T07:16:45.339437Z",
     "shell.execute_reply.started": "2023-05-06T07:16:44.751282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 500ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prob_of_toxic_comment': 0.368846,\n",
       " 'prob_of_non_toxic_comment': 0.6311540007591248}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_of_toxic_comment = loaded_model.predict(prep_data(text=text, tokenizer=tokenizer_, max_len=192))[0][0]\n",
    "prob_of_non_toxic_comment = 1 - prob_of_toxic_comment\n",
    "prob_of_toxic_comment, prob_of_non_toxic_comment\n",
    "probs = {\"prob_of_toxic_comment\": prob_of_toxic_comment,\n",
    " \"prob_of_non_toxic_comment\": prob_of_non_toxic_comment}\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
